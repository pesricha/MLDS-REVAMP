\documentclass{beamer}
\usepackage{amsmath, amssymb, graphicx}
\usepackage{bm} % For bold math symbols

\usetheme{Madrid}

\title{Principal Component Analysis}
\author{Naman Pesricha}
\date{\today}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

% Slide 1: Notations
\begin{frame}{Notations}
    [Placeholder]
\end{frame}

\begin{frame}{Intuition - PCA}
    [Placeholder]
\end{frame}

\begin{frame}{Mathematical Foundations: \textbf{\underline{Symmetric}} Positive Definite Matrices and KKT Conditions}

\begin{block}{Properties of Symmetric Matrices}
    Let $\bm{A} \in \mathbb{R}^{n \times n}$ be a symmetric matrix ($\bm{A} = \bm{A}^{\top}$).
    
    \begin{itemize}
        \item $\bm{A} = \bm{A}^{\top}$ (by definition of symmetry)
        \item All eigenvalues of $\bm{A}$ are real
        \item Eigenvectors of symmetric matrices can be chosen to form an orthogonal set
    \end{itemize}
\end{block}

\vspace{0.3cm}

\textbf{Proof: Eigenvalues of $\bm{A}$ are real}  

Let $\lambda \in \mathbb{C}$ and $\bm{v} \in \mathbb{C}^n$ be an eigenpair:
\[
\bm{A}\bm{v} = \lambda \bm{v}, \quad \bm{v} \neq \bm{0}
\]
Consider $\bm{v}^* \bm{A} \bm{v}$:
\[
\bm{v}^* \bm{A} \bm{v} = \bm{v}^* (\lambda \bm{v}) = \lambda \bm{v}^* \bm{v}
\]

\end{frame}

\begin{frame}{Mathematical Foundations: \textbf{\underline{Symmetric}} Positive Definite Matrices and KKT Conditions}

Taking conjugate transpose of $\bm{A}\bm{v} = \lambda \bm{v}$:
\[
(\bm{A}\bm{v})^* = (\lambda \bm{v})^* \quad \Rightarrow \quad \bm{v}^* \bm{A} = \bar{\lambda} \bm{v}^*
\]
Thus:
\[
\bm{v}^* \bm{A} \bm{v} = \bar{\lambda} \bm{v}^* \bm{v}
\]
Equating both expressions:
\[
\lambda \bm{v}^* \bm{v} = \bar{\lambda} \bm{v}^* \bm{v}
\]
Since $\bm{v}^* \bm{v} > 0$, we get $\boxed{\lambda = \bar{\lambda} \quad \Rightarrow \quad \lambda \in \mathbb{R}}$

\end{frame}

\begin{frame}{Mathematical Foundations: \textbf{\underline{Symmetric}} Positive Definite Matrices and KKT Conditions}

\textbf{Proof: Eigenvectors of symmetric matrices can be chosen to form an orthogonal set}

Let $\bm{A} \in \mathbb{R}^{n \times n}$ be symmetric and let $\lambda_1 \neq \lambda_2$ with corresponding eigenvectors $\bm{v}_1$ and $\bm{v}_2$.

We have:
\[
\bm{A}\bm{v}_1 = \lambda_1 \bm{v}_1, \quad \bm{A}\bm{v}_2 = \lambda_2 \bm{v}_2
\]
Consider:
\[
\bm{v}_2^{\top} \bm{A} \bm{v}_1 = \bm{v}_2^{\top} (\lambda_1 \bm{v}_1) = \lambda_1 \bm{v}_2^{\top} \bm{v}_1
\]
Using symmetry ($\bm{A} = \bm{A}^{\top}$):
\[
\bm{v}_2^{\top} \bm{A} \bm{v}_1 = (\bm{A} \bm{v}_2)^{\top} \bm{v}_1 = (\lambda_2 \bm{v}_2)^{\top} \bm{v}_1 = \lambda_2 \bm{v}_2^{\top} \bm{v}_1
\]

\end{frame}

\begin{frame}{Mathematical Foundations: \textbf{\underline{Symmetric}} Positive Definite Matrices and KKT Conditions}

Thus:
\[
\lambda_1 \bm{v}_2^{\top} \bm{v}_1 = \lambda_2 \bm{v}_2^{\top} \bm{v}_1
\]
Since $\lambda_1 \neq \lambda_2$, we conclude:
\[
\boxed{\bm{v}_2^{\top} \bm{v}_1 = 0 \quad \Rightarrow \quad \bm{v}_1 \perp \bm{v}_2}
\]

If some eigenvalues are repeated, we can still choose orthogonal eigenvectors within the corresponding eigenspace. These are also orthogonal to eigenvectors of distinct eigenvalues. The proof for this is beyond the scope of this course. Interested students can contact the TAs.
\vspace{0.3cm}
Thus, the proof is complete.

\end{frame}

\begin{frame}{Mathematical Foundation: \textbf{\underline{Symmetric Positive Definite}} Matrices and KKT Conditions}

\begin{block}{Definition: Symmetric Positive Definite (PD) Matrix}

A symmetric matrix $\bm{A} \in \mathbb{R}^{n \times n}$ is \textbf{positive definite} if:
\[
\bm{A} = \bm{A}^\top \quad \text{and} \quad \bm{v}^\top \bm{A} \bm{v} > 0 \quad \forall \, \bm{v} \in \mathbb{R}^n, \, \bm{v} \neq \bm{0}
\]

If $\bm{v}^\top \bm{A} \bm{v} \geq 0$ for all $\bm{v} \neq \bm{0}$, then $\bm{A}$ is \textbf{positive semi-definite (PSD)}.

\vspace{0.3cm}

\textbf{Key properties:}
\begin{itemize}
    \item $\bm{A} = \bm{A}^\top$ (symmetric)
    \item $\bm{v}^\top \bm{A} \bm{v} > 0$ for all $\bm{v} \neq \bm{0}$
    \item All eigenvalues are real $\lambda_i > 0$ $\forall i$
    \item Eigenvectors of PD matrices can be chosen to form an orthogonal set
\end{itemize}

\end{block}

\end{frame}

\begin{frame}{Mathematical Foundation: \textbf{\underline{Symmetric Positive Definite}} Matrices and KKT Conditions}

\textbf{Proof: For a symmetric positive definite matrix, all eigenvalues are positive}

Let $\lambda$ be an eigenvalue with eigenvector $\bm{v} \neq \bm{0}$:
\[
\bm{A} \bm{v} = \lambda \bm{v}
\]
Consider:
\[
\bm{v}^\top \bm{A} \bm{v} = \bm{v}^\top (\lambda \bm{v}) = \lambda \bm{v}^\top \bm{v}
\]
Since $\bm{v}^\top \bm{v} > 0$ and $\bm{v}^\top \bm{A} \bm{v} > 0$, it follows:
\[
\lambda \bm{v}^\top \bm{v} > 0 \quad \Rightarrow \quad \lambda > 0
\]
Thus:
\[
\boxed{\lambda_i > 0 \quad \forall \, i = 1,2,\dots, n}
\]

\end{frame}


\begin{frame}{Mathematical Foundation: \textbf{\underline{Symmetric Positive Definite}} Matrices and KKT Conditions}

\begin{block}{Definition: Symmetric Positive Semi-Definite (PSD) Matrix}

A symmetric matrix $\bm{A} \in \mathbb{R}^{n \times n}$ is \textbf{positive semi-definite} if:
\[
\bm{A} = \bm{A}^\top \quad \text{and} \quad \bm{v}^\top \bm{A} \bm{v} \geq 0 \quad \forall \, \bm{v} \in \mathbb{R}^n, \, \bm{v} \neq \bm{0}
\]

\vspace{0.3cm}

\textbf{Key properties:}
\begin{itemize}
    \item $\bm{A} = \bm{A}^\top$ (symmetric)
    \item $\bm{v}^\top \bm{A} \bm{v} \geq 0$ for all $\bm{v} \neq \bm{0}$
    \item All eigenvalues are real $\lambda_i \geq 0$ $\forall i$
    \item Eigenvectors can be chosen to form an orthogonal set
\end{itemize}

\end{block}

\end{frame}

\begin{frame}{Mathematical Foundation: \textbf{\underline{Symmetric Positive Definite}} Matrices and KKT Conditions}

\begin{alertblock}{Comparison: PD vs PSD Matrices}

\begin{tabular}{ll}
\textbf{PD} & \textbf{PSD} \\
\hline
\hfill \\
$\bm{v}^\top \bm{A} \bm{v} > 0$ & $\bm{v}^\top \bm{A} \bm{v} \geq 0$ \\
$\lambda_i > 0$ & $\lambda_i \geq 0$ \\
Invertible & May be singular \\
$\det(\bm{A}) > 0$ & $\det(\bm{A}) \geq 0$ \\
Strictly convex quadratic form & Convex quadratic form \\
Cholesky factorization exists & LDL$^\top$ factorization exists \\
\end{tabular}

\vspace{0.3cm}

Note: Every PD matrix is PSD, but not conversely.

\end{alertblock}

\end{frame}

% BEGIN KKT

\begin{frame}{{Mathematical Foundation: Symmetric Positive Definite Matrices and \textbf{\underline{KKT Conditions}}}}
\begin{block}{Problem Setup: General Constrained Optimization Problem}
\[
\text{Min } f(\bm{x}) \quad \text{s.t.} \quad
\begin{cases}
g_j(\bm{x}) \leq 0 & j = 1,\ldots,J \\
h_k(\bm{x}) = 0 & k = 1,\ldots,K
\end{cases}
\]

\begin{itemize}
\item \textbf{Decision variables}: $\bm{x} \in \mathbb{R}^n$
\item \textbf{Objective}: $f: \mathbb{R}^n \rightarrow \mathbb{R}$
\item \textbf{Feasible set}: $\{\bm{x} | g_j(\bm{x}) \leq 0, h_k(\bm{x}) = 0\}$
\end{itemize}
\end{block}

\textbf{Constraint Classification}
\begin{itemize}
\item \textbf{ACTIVE} : Constraints that can be violated with an infinitesimal step in the wrong direction.
\item \textbf{INACTIVE} : Constraints which won't get violated for any infinitesimally small step.
\end{itemize}
\end{frame}

\begin{frame}{{Mathematical Foundation: Symmetric Positive Definite Matrices and \textbf{\underline{KKT Conditions}}}}

\begin{block}{Constraint Sets}
For feasible $\bm{x}^*$:

\begin{itemize}
\item \textbf{Active constraints} ($Ac$):
\begin{itemize}
\item Inequalities: $Ac_I = \{j \mid g_j(\bm{x}^*) = 0\}$
\item Equalities: $Ac_E = \{1,\ldots,K\}$ (always active)
\end{itemize}

\item \textbf{Inactive constraints} ($Ic$):
\begin{itemize}
\item Inequalities: $Ic = \{j \mid g_j(\bm{x}^*) < 0\}$
\end{itemize}
\end{itemize}
\end{block}

\begin{block}{Regularity Condition}
$\bm{x}^*$ is \textbf{regular} iff:
\[
\mathrm{rank}\left(\{\nabla g_j(\bm{x}^*)\}_{j \in Ac_I} \cup \{\nabla h_k(\bm{x}^*)\}_{k \in Ac_E}\right) = |Ac_I| + |Ac_E|
\]
i.e., the active constraint gradients are linearly independent.
\end{block}


\end{frame}

\begin{frame}{{Mathematical Foundation: Symmetric Positive Definite Matrices and \textbf{\underline{KKT Conditions}}}}
\begin{block}{Lagrangian Function}
\[
\mathcal{L}(\bm{x}, \bm{\mu}, \bm{\lambda}) = f(\bm{x}) + \underbrace{\sum_{j=1}^J \mu_j g_j(\bm{x})}_{\text{inequalities}} + \underbrace{\sum_{k=1}^K \lambda_k h_k(\bm{x})}_{\text{equalities}}
\]
\begin{itemize}
\item $\lambda_k \in \mathbb{R}$: Equality multipliers
\item $\mu_j \geq 0$: Inequality multipliers
\end{itemize}
\end{block}
\end{frame}


\begin{frame}{{Mathematical Foundation: Symmetric Positive Definite Matrices and \textbf{\underline{KKT Conditions}}}}
\begin{block}{KKT Conditions Part I}
For regular local minimum $\bm{x}^*$:

1. \textbf{Stationarity}:
Derivative wrt $\bm{x}$ is 0 at $\bm{x}$ = $\bm{x}^*$.

\[
\nabla_{\bm{x}} \mathcal{L}(\bm{x}^*, \bm{\mu}, \bm{\lambda}) = \bm{0}
\]
\[
\Rightarrow \nabla f(\bm{x}^*) + \sum_{j=1}^J \mu_j \nabla g_j(\bm{x}^*) + \sum_{k=1}^K \lambda_k \nabla h_k(\bm{x}^*) = \bm{0}
\]


2. \textbf{Primal Feasibility}:
All constraints should be satisfied at $\bm{x^*}$.
\[
g_j(\bm{x}^*) \leq 0 \quad \forall j, \quad h_k(\bm{x}^*) = 0 \quad \forall k
\]

\end{block}
\end{frame}

\begin{frame}{{Mathematical Foundation: Symmetric Positive Definite Matrices and \textbf{\underline{KKT Conditions}}}}
\begin{block}{KKT Conditions Part II}

3. \textbf{Dual Feasibility}: \\
Dual feasibility ($\mu_j \geq 0$) ensures the gradient balance $\nabla f = -\sum \mu_j \nabla g_j$ opposes constraint violations, preventing descent into infeasible regions.
\[
\mu_j \geq 0 \quad \forall j = 1,\ldots,J
\]


4. \textbf{Complementary Slackness}:
\[
\mu_j g_j(\bm{x}^*) = 0 \quad \forall j
\]
\begin{itemize}
\item Implies $\mu_j = 0$ for inactive constraints
\item Active constraints may have $\mu_j > 0$.\\ 
\end{itemize}
\end{block}

\end{frame}

\begin{frame}{{Mathematical Foundation: Symmetric Positive Definite Matrices and \textbf{\underline{KKT Conditions}}}}
\begin{block}{Optimality Conditions}

\begin{itemize}
\item  KKT conditions are \textbf{necessary} for local optimality at regular points
\item  Regularity requires linear independence of active constraint gradients
\item  For convex problems (convex $f$, convex $g_j$, affine $h_k$), KKT becomes \textbf{sufficient}
\item  In convex cases, KKT solutions are \textbf{global minima}
\item  Complementary slackness reveals which constraints are active ($\mu_j > 0$)
\end{itemize}

\end{block}

\begin{alertblock}{Key Insight}
KKT serves as both optimality \textbf{verification} (necessary in non-convex) and \textbf{certification} (sufficient in convex cases)
\end{alertblock}
\end{frame}


% END KKT



\end{document}
